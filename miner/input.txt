This lecture is about the Probabilistic Retrieval Model. In this lecture were going to continue the discussion of the Text Retrieval Methods. Were going to look at another kind of very different way to design ranking functions than the Vector Space Model that we discussed before. In probabilistic models we define the ranking function based on the probability that this document is relevant to this query. In other words we introduce a binary random variable here. This is the variable R here. And we also assume that the query and the documents are all observations from random variables. Note that in the vector based models we assume they are vectors but here we assume they are the data observed from random variables. And so the problem of retrieval becomes to estimate the probability of relevance. In this category of models there are different variants. The classic probabilistic model has led to the BM25 retrieval function which we discussed in in the vectors based model because its a form is actually similar to a backwards space model. In this lecture we will discuss another sub class in this P class called a language modeling approaches to retrieval. In particular were going to discuss the query likelihood retrieval model which is one of the most effective models in probabilistic models. There was also another line called the divergence from randomness model which has led to the PL2 function its also one of the most effective state of the art retrieval functions. In query likelihood our assumption is that this probability of relevance can be approximated by the probability of query given a document and relevance. So intuitively this probability just captures the following probability. And that is if a user likes document d how likely would the user enter query q in order to retrieve document d? So we assume that the user likes d because we have a relevance value here. And then we ask the question about how likely well see this particular query from this user? So this is the basic idea. Now to understand this idea lets take a look at the general idea or the basic idea of Probabilistic Retrieval Models. So here I listed some imagined relevance status values or relevance judgments of queries and documents. For example in this line it shows that q1 is a query that the user typed in. And d1 is a document that the user has seen. And 1 means the user thinks d1 is relevant to q1. So this R here can be also approximated by the click through data that a search engine can collect by watching how you interacted with the search results. So in this case lets say the user clicked on this document. So theres a 1 here. Similarly the user clicked on d2 also so there is a 1 here. In other words d2 is assumed to be relevant to q1. On the other hand d3 is non relevant theres a 0 here. And d4 is non relevant and then d5 is again relevant and so on and so forth. And this part maybe data collected from a different user. So this user typed in q1 and then found that the d1 is actually not useful so d1 is actually non relevant. In contrast here we see its relevant. Or this could be the same query typed in by the same user at different times. But d2 is also relevant etc. And then here we can see more data about other queries. Now we can imagine we have a lot of such data. Now we can ask the question how can we then estimate the probability of relevance? So how can we compute this probability of relevance? Well intuitively that just means if we look at all the entries where we see this particular d and this particular q how likely well see a one on this other column. So basically that just means that we can just collect the counts. We can first count how many times we have seen q and d as a pair in this table and then count how many times we actually have also seen 1 in the third column. And then we just compute the ratio. So lets take a look at some specific examples. Suppose we are trying to compute this probability for d1 d2 and d3 for q1. What is the estimated probability? Now think about that. You can pause the video if needed. Try to take a look at the table. And try to give your estimate of the probability. Have you seen that if we are interested in q1 and d1 well be looking at these two pairs? And in both cases well actually in one of the cases the user has said this is 1 this is relevant. So R  1 in only one of the two cases. In the other case its 0. So thats one out of two. What about the d1 and the d2? Well they are here d1 and d2 d1 and d2 in both cases in this case R  1. So its a two out of two and so on and so forth. So you can see with this approach we can actually score these documents for the query right? We now have a score for d1 d2 and d3 for this query. And we can simply rank them based on these probabilities and so thats the basic idea probabilistic retrieval model. And you can see it makes a lot of sense in this case its going to rank d2 above all the other documents. Because in all the cases when you have c and q1 and d2 R  1. The user clicked on this document. So this also should show that with a lot of click through data a search engine can learn a lot from the data to improve their search engine. This is a simple example that shows that with even with small amount of entries here we can already estimate some probabilities. These probabilities would give us some sense about which document might be more relevant or more useful to a user for typing this query. Now of course the problems that we dont observe all the queries and all the documents and all the relevance values right? There would be a lot of unseen documents in general we have only collected the data from the documents that we have shown to the users. And there are even more unseen queries because you cannot predict what queries will be typed in by users. So obviously this approach wont work if we apply it to unseen queries or unseen documents. Nevertheless this shows the basic idea of probabilistic retrieval model and it makes sense intuitively. So what do we do in such a case when we have a lot of unseen documents and unseen queries? Well the solutions that we have to approximate in some way. So in this particular case called a query likelihood retrieval model we just approximate this by another conditional probability. pq given d R1. So in the condition part we assume that the user likes the document because we have seen that the user clicked on this document. And this part shows that were interested in how likely the user would actually enter this query. How likely we will see this query in the same row. So note that here we have made an interesting assumption here. Basically were going to do assume that whether the user types in this query has something to do with whether user likes the document. In other words we actually make the following assumption. And that is a user formulates a query based on an imaginary relevant document. Where if you just look at this as conditional probability its not obvious we are making this assumption. So what I really meant is that to use this new conditional probability to help us score then this new conditional probability will have to somehow be able to estimate this conditional probability without relying on this big table. Otherwise we would be having similar problems as before and by making this assumption we have some way to bypass this big table and try to just model how the user formulates the query okay? So this is how you can simplify the general model so that we can derive a specific relevant function later. So lets look at how this model work for our example. And basically what we are going to do in this case is to ask the following question. Which of these documents is most likely the imaginary relevant document in the users mind when the user formulates this query? So we ask this question and we quantify the probability and this probability is a conditional probability of observing this query if a particular document is in fact the imaginary relevant document in the users mind. Here you can see weve computed all these query likelihood probabilities. The likelihood of queries given each document. Once we have these values we can then rank these documents based on these values. So to summarize the general idea of modern relevance in the proper risk model is to assume the we introduce a binary random variable R here. And then let the scoring function be defined based on this conditional probability. We also talked about approximating this by using the query likelihood. And in this case we have a ranking function thats basically based on the probability of a query given the document. And this probability should be interpreted as the probability that a user who likes document d would pose query q. Now the question of course is how do we compute this conditional probability? At this in general has to do with how you compute the probability of text because q is a text. And this has to do with a model called a Language Model. And these kind of models are proposed to model text. So more specifically we will be very interested in the following conditional probability as is shown in this here. If the user liked this document how likely the user would pose this query. And in the next lecture were going to do giving introduction to language models that we can see how we can model text that was a probable risk model in general.  This lecture is about the statistical language model. In this lecture were going to give an introduction to statistical language model. This has to do with how do you model text data with probabilistic models. So its related to how we model query based on a document. Were going to talk about what is a language model. And then were going to talk about the simplest language model called the unigram language model which also happens to be the most useful model for text retrieval. And finally what this class will use is a language model. What is a language model? Well its just a probability distribution over word sequences. So here Ill show one. This model gives the sequence Today is Wednesday a probability of 0. 001. It give Today Wednesday is a very very small probability because its non grammatical. You can see the probabilities given to these sentences or sequences of words can vary a lot depending on the model. Therefore its clearly context dependent. In ordinary conversation probably Today is Wednesday is most popular among these sentences. Imagine in the context of discussing apply the math maybe the eigenvalue is positive would have a higher probability. This means it can be used to represent the topic of a text. The model can also be regarded as a probabilistic mechanism for generating text. And this is why its also often called a generating model. So what does that mean? We can imagine this is a mechanism thats visualised here as a stochastic system that can generate sequences of words. So we can ask for a sequence and its to send for a sequence from the device if you want and it might generate for example Today is Wednesday but it could have generated any other sequences. So for example there are many possibilities right? So in this sense we can view our data as basically a sample observed from such a generating model. So why is such a model useful? Well its mainly because it can quantify the uncertainties in natural language. Where do uncertainties come from? Well one source is simply the ambiguity in natural language that we discussed earlier in the lecture. Another source is because we dont have complete understanding we lack all the knowledge to understand the language. In that case there will be uncertainties as well. So let me show some examples of questions that we can answer with a language model that would have interesting applications in different ways. Given that we see John and feels how likely will we see happy as opposed to habit as the next word in a sequence of words? Now obviously this would be very useful for speech recognition because happy and habit would have similar acoustic sound acoustic signals. But if we look at the language model we know that John feels happy would be far more likely than John feels habit. Another example given that we observe baseball three times and game once in a news article how likely is it about sports? This obviously is related to text categorization and information retrieval. Also given that a user is interested in sports news how likely would the user use baseball in a query? Now this is clearly related to the query likelihood that we discussed in the previous lecture. So now lets look at the simplest language model called a unigram language model. In such a case we assume that we generate a text by generating each word independently. So this means the probability of a sequence of words would be then the product of the probability of each word. Now normally theyre not independent right? So if you have single word in like a language that would make it far more likely to observe model than if you havent seen the language. So this assumption is not necessarily true but we make this assumption to simplify the model. So now the model has precisely N parameters where N is vocabulary size. We have one probability for each word and all these probabilities must sum to 1. So strictly speaking we actually have N 1 parameters. As I said text can then be assumed to be assembled drawn from this word distribution. So for example now we can ask the device or the model to stochastically generate the words for us instead of sequences. So instead of giving a whole sequence like Today is Wednesday it now gives us just one word. And we can get all kinds of words. And we can assemble these words in a sequence. So that will still allow you to compute the probability of Today is Wednesday as the product of the three probabilities. As you can see even though we have not asked the model to generate the sequences it actually allows us to compute the probability for all the sequences but this model now only needs N parameters to characterize. That means if we specify all the probabilities for all the words then the models behavior is completely specified. Whereas if we dont make this assumption we would have to specify probabilities for all kinds of combinations of words in sequences. So by making this assumption it makes it much easier to estimate these parameters. So lets see a specific example here. Here I show two unigram language models with some probabilities. And these are high probability words that are shown on top. The first one clearly suggests a topic of text mining because the high probability was all related to this topic. The second one is more related to health. Now we can ask the question how likely were observe a particular text from each of these two models? Now suppose we sample words to form a document. Lets say we take the first distribution would you like to sample words? What words do you think would be generated while making a text or maybe mining maybe another word? Even food which has a very small probability might still be able to show up. But in general high probability words will likely show up more often. So we can imagine what general text of that looks like in text mining. In fact with small probability you might be able to actually generate the actual text mining paper. Now it will actually be meaningful although the probability will be very very small. In an extreme case you might imagine we might be able to generate a text mining paper that would be accepted by a major conference. And in that case the probability would be even smaller. But its a non zero probability if we assume none of the words have non zero probability. Similarly from the second topic we can imagine we can generate a food nutrition paper. That doesnt mean we cannot generate this paper from text mining distribution. We can but the probability would be very very small maybe smaller than even generating a paper that can be accepted by a major conference on text mining. So the point is that the keeping distribution we can talk about the probability of observing a certain kind of text. Some texts will have higher probabilities than others. Now lets look at the problem in a different way. Suppose we now have available a particular document. In this case many of the abstract or the text mining table and we see these word counts here. The total number of words is 100. Now the question you ask here is an estimation question. We can ask the question which model which one of these distribution has been used to generate this text assuming that the text has been generated by assembling words from the distribution. So what would be your guess? What we have to decide are what probabilities text mining etc. would have. Suppose the view for a second and try to think about your best guess. If youre like a lot of people you would have guessed that well my best guess is text has a probability of 10 out of 100 because Ive seen text 10 times and there are in total 100 words. So we simply normalize these counts. And thats in fact the word justified and your intuition is consistent with mathematical derivation. And this is called the maximum likelihood estimator. In this estimator we assume that the parameter settings of those that would give our observe the data the maximum probability. That means if we change these probabilities then the probability of observing the particular text data would be somewhat smaller. So you can see this has a very simple formula. Basically we just need to look at the count of a word in a document and then divide it by the total number of words in the document or document lens. Normalize the frequency. A consequence of this is of course were going to assign zero probabilities to unseen words. If we have an observed word there will be no incentive to assign a non zero probability using this approach. Why? Because that would take away probability mass for these observed words. And that obviously wouldnt maximize the probability of this particular observed text data. But one has still question whether this is our best estimate. Well the answer depends on what kind of model you want to find right? This estimator gives a best model based on this particular data. But if you are interested in a model that can explain the content of the full paper for this abstract then you might have a second thought right? So for thing there should be other words in the body of that article so they should not have zero probabilities even though theyre not observed in the abstract. So were going to cover this a little bit more later in this class in the query likelihood model. So lets take a look at some possible uses of these language models. One use is simply to use it to represent the topics. So here I show some general English background texts. We can use this text to estimate a language model and the model might look like this. Right so on the top we have those all common words the a is we etc. and then well see some common words like these and then some very very rare words in the bottom. This is a background language model. It represents the frequency of words in English in general. This is the background model. Now lets look at another text maybe this time well look at the computer science research papers. So we have a collection of computer science research papers we do as mentioned again we can just use the maximum likelihood estimator where we simply normalize the frequencies. Now in this case well get the distribution that looks like this. On the top it looks similar because these words occur everywhere they are very common. But as we go down well see words that are more related to computer science computer software text etc. And so although here we might also see these words for example computer but we can imagine the probability here is much smaller than the probability here. And we will see many other words here that would be more common in general English. So you can see this distribution characterizes a topic of the corresponding text. We can look at even the smaller text. So in this case lets look at the text mining paper. Now if we do the same we have another distribution again the can be expected to occur in the top. The sooner we see text mining association clustering these words have relatively high probabilities. In contrast in this distribution the text has a relatively small probability. So this means again based on different text data we can have a different model and the model captures the topic. So we call this document the language model and we call this collection language model. And later you will see how theyre used in the retrieval function. But now lets look at another use of this model. Can we statistically find what words are semantically related to computer? Now how do we find such words? Well our first thought is that lets take a look at the text that match computer. So we can take a look at all the documents that contain the word computer. Lets build a language model. We can see what words we see there. Well not surprisingly we see these common words on top as we always do. So in this case this language model gives us the conditional probability of seeing the word in the context of computer. And these common words will naturally have high probabilities. But we also see the computer itself and software will have relatively high probabilities. But if we just use this model we cannot just say all these words are semantically related to computer. So ultimately what wed like to get rid of is these common words. How can we do that? It turns out that its possible to use language model to do that. But I suggest you think about that. So how can we know what words are very common so that we want to kind of get rid of them? What model will tell us that? Well maybe you can think about that. So the background language model precisely tells us this information. It tells us what was our common in general. So if we use this background model we would know that these words are common words in general. So its not surprising to observe them in the context of computer. Whereas computer has a very small probability in general so its very surprising that we have seen computer with this probability and the same is true for software. So then we can use these two models to somehow figure out the words that are related to computer. For example we can simply take the ratio of these group probabilities and normalize the topic of language model by the probability of the word in the background language model. So if we do that we take the ratio well see that then on the top computer is ranked and then followed by software program all these words related to computer. Because they occur very frequently in the context of computer but not frequently in the whole collection whereas these common words will not have a high probability. In fact they have a ratio about 1 down there because they are not really related to computer. By taking the sample of text that contains the computer we dont really see more occurrences of that than in general. So this shows that even with these simple language models we can do some limited analysis of semantics. So in this lecture we talked about language model which is basically a probability distribution over text. We talked about the simplest language model called unigram language model which is also just a word distribution. We talked about the two uses of a language model. One is we represent the topic in a document in a collection or in general. The other is we discover word associations. In the next lecture were going to talk about how language model can be used to design a retrieval function. Here are two additional readings. The first is a textbook on statistical natural language processing. The second is an article that has a survey of statistical language models with a lot of pointers to research work.  This lecture is about query likelihood probabilistic retrieval model. In this lecture we continue the discussion of probabilistic retrieval model. In particular were going to talk about the query light holder retrieval function. In the query light holder retrieval model our idea is model. How like their user who likes a document with pose a particular query? So in this case you can imagine if a user likes this particular document about a presidential campaign news. Now we assume the user would use this a document as a basis to impose a query to try and retrieve this document. So again imagine use a process that works as follows. Where we assume that the query is generated by assembling words from the document. So for example a user might pick a word like presidential from this document and then use this as a query word. And then the user would pick another word like campaign and that would be the second query word. Now this of course is an assumption that we have made about how a user would pose a query. Whether a user actually followed this process may be a different question but this assumption has allowed us to formerly characterize this conditional probability. And this allows us to also not rely on the big table that I showed you earlier to use empirical data to estimate this probability. And this is why we can use this idea then to further derive retrieval function that we can implement with the program language. So as you see the assumption that we made here is each query word is independent of the sample. And also each word is basically obtained from the document. So now lets see how this works exactly. Well since we are completing a query likelihood then the probability here is just the probability of this particular query which is a sequence of words. And we make the assumption that each word is generated independently. So as a result the probability of the query is just a product of the probability of each query word. Now how do we compute the probability of each query word? Well based on the assumption that a word is picked from the document that the user has in mind. Now we know the probability of each word is just the relative frequency of each word in the document. So for example the probability of presidential given the document. Would be just the count of presidential document divided by the total number of words in the document or document s. So with these assumptions we now have actually a simple formula for retrieval. We can use this to rank our documents. So does this model work? Lets take a look. Here are some example documents that you have seen before. Suppose now the query is presidential campaign and we see the formula here on the top. So how do we score this document? Well its very simple. We just count how many times do we have seen presidential or how many times do we have seen campaigns etc. And we see here 44 and weve seen presidential twice. So thats 2 over the length of document 4 multiplied by 1 over the length of document 4 for the probability of campaign. And similarly we can get probabilities for the other two documents. Now if you look at these numbers or these formulas for scoring all these documents it seems to make sense. Because if we assume d3 and d4 have about the same length then looks like a nominal rank d4 above d3 and which is above d2. And as we would expect looks like it did captures a TF query state and so this seems to work well. However if we try a different query like this one presidential campaign update then we might see a problem. Well what problem? Well think about the update. Now none of these documents has mentioned update. So according to our assumption that a user would pick a word from a document to generate a query then the probability of obtaining the word update would be what? Would be 0. So that causes a problem because it would cause all these documents to have zero probability of generating this query. Now why its fine to have zero probability for d2 which is non relevant? Its not okay to have 0 for d3 and d4 because now we no longer can distinguish them. Whats worse? We cant even distinguish them from d2. So thats obviously not desirable. Now when a INAUDIBLE has such result we should think about what has caused this problem? So we have to examine what assumptions have been made as we derive this ranking function. Now is you examine those assumptions carefully you will realize what has caused this problem? So take a moment to think about it. What do you think is the reason why update has zero probability and how do we fix it? So if you think about this from the moment you realize that thats because we have made an assumption that every query word must be drawn from the document in the users mind. So in order to fix this we have to assume that the user could have drawn a word not necessarily from the document. So thats the improved model. An improvement here is to say that well instead of drawing a word from the document lets imagine that the user would actually draw a word from a document model. And so I show a model here. And we assume that this document is generated using this unigram language model. Now this model doesnt necessarily assign zero probability for update in fact we can assume this model does not assign zero probability for any word. Now if were thinking this way then the generation process is a little bit different. Now the user has this model in mind instead of this particular document. Although the model has to be estimated based on the document. So the user can again generate the query using a singular process. Namely pick a word for example presidential and another word campaign. Now the difference is that this time we can also pick a word like update even though update doesnt occur in the document to potentially generate a query word like update. So that a query was updated 1 times 0 probabilities. So this would fix our problem. And its also reasonable because when our thinking of what the user is looking for in a more general way that is unique language model instead of fixed document. So how do we compute this query likelihood? If we make this sum wide involved two steps. The first one is compute this model and we call it document language model here. For example Ive shown two pulse models here its major based on two documents. And then given a query like a data mining algorithms the thinking is that well just compute the likelihood of this query. And by making independence assumptions we could then have this probability as a product of the probability of each query word. We do this for both documents and then we can score these two documents and then rank them. So thats the basic idea of this query likelihood retrieval function. So more generally this ranking function would look like in the following. Here we assume that the query has n words w1 through wn and then the scoring function. The ranking function is the probability that we observe this query given that the user is thinking of this document. And this is assume it will be product of probabilities of all individual words. This is based on independent assumption. Now we actually often score the document before this query by using log of the query likelihood as shown on the second line. Now we do this to avoid having a lot of small probabilities mean multiply together. And this could cause under flow and we might loose the precision by transforming the value in our algorithm function. We maintain the order of these documents yet we can avoid the under flow problem. And so if we take longer than transformation of course the product would become a sum as you on the second line here. So the sum of all the query words inside of the sum that is one of the probability of this word given by the document. And then we can further rewrite the sum to a different form. So in the first sum here in this sum we have it over all the query words and query word. And in this sum we have a sum of all the possible words. But we put a counter here of each word in the query. Essentially we are only considering the words in the query because if a word is not in the query the count will be 0. So were still considering only these n words. But were using a different form as if we were going to take a sample of all the words in the vocabulary. And of course a word might occur multiple times in the query. Thats why we have a count here. And then this part is log of the probability of the word given by the document language model. So you can see in this retrieval function we actually know the count of the word in the query. So the only thing that we dont know is this document language model. Therefore we have converted the retrieval problem include the problem of estimating this document language model. So that we can compute the probability of each query word given by this document. And different estimation methods would lead to different ranking functions. This is just like a different way to place document in the vector space which leads to a different ranking function in the vector space model. Here different ways to estimate will lead to a different ranking function for query likelihood.  This lecture is about smoothing of language models. In this lecture were going to continue talking about the probabilistic retrieval model. In particular were going to talk about the smoothing of language model in the query likelihood retrieval method. So you have seen this slide from a previous lecture. This is the ranking function based on the query likelihood. Here we assume that the independence of generating each query word And the formula would look like the following where we take a sum of all the query words. And inside the sum there is a log of probability of a word given by the document or document image model. So the main task now is to estimate this document language model as we said before different methods for estimating this model would lead to different retrieval functions. So in this lecture were going to be looking to this in more detail. So how do we estimate this language model? Well the obvious choice would be the maximum likelihood estimate that we have seen before. And that is were going to normalize the word frequencies in the document. And estimate the probability it would look like this. This is a step function here. Which means all of the words that have the same frequency count will have identical problem with it. This is another freedom to count that has different probability. Note that for words that have not occurred in the document here they will have 0 probability. So we know this is just like the model that we assume earlier in the lecture. Where we assume that the use of the simple word from the document to a formula to clear it. And theres no chance of assembling any word thats not in the document and we know thats not good. So how do we improve this? Well in order to assign a none 0 probability to words that have not been observed in the document we would have to take away some probability mass from the words that are observed in the document. So for example here we have to take away some probability of the mass because we need some extra probability mass for the words otherwise they wont sum to 1. So all these probabilities must sum to 1. So to make this transformation and to improve the maximum likelihood estimated by assigning non zero probabilities to words that are not observed in the data. We have to do smoothing and smoothing has to do with improving the estimate by considering the possibility that if the author had been asking to write more words for the document the author might have written other words. If you think about this factor then the a smoothed language model would be a more accurate than the representation of the actual topic. Imagine you have seen an abstract of a research article. Lets say this document is abstract. If we assume and see words in this abstract that we have a probability of 0. That would mean theres no chance of sampling a word outside the abstract of the formulated query. But imagine a user who is interested in the topic of this subject. The user might actually choose a word thats not in that chapter to use as query. So obviously if we has asked this author to write more author would have written a full text of the article. So smoothing of the language model is an attempt to try to recover the model for the whole article. And then of course we dont have knowledge about any words that are not observed in the abstract. So thats why smoothing is actually a tricky problem. So lets talk a little more about how to smooth a language model. The key question here is what probability should be assigned to those unseen words? And there are many different ways of doing that. One idea here thats very useful for retrieval is let the probability of unseen word be proportional to its probability given by a reference language model. That means if you dont observe the word in the dataset. Were going to assume that its probability is kind of governed by another reference language model that we will construct. It will tell us which unseen words would have a higher probability. In the case of retrieval a natural choice would be to take the collection language model as the reference language model. That is to say if you dont observe a word in the document were going to assume that the probability of this word would be proportional to the probability of word in the whole collection. So more formally well be estimating the probability of a word key document as follows. If the word is seen in the document then the probability would be this counted the maximum likelihood estimate P sub c here. Otherwise if the word is not seen in the document were going to let probability be proportional to the probability of the word in the collection. And here the coefficient that offer is to control the amount of probability mass that we assign to unseen words. Obviously all these probabilities must sum to 1 so alpha sub d is constrained in some way. So what if we plug in this smoothing formula into our query likelihood ranking function? This is what we will get. In this formula we have this as a sum over all the query words and those that we have written here as the sum of all the vocabulary you see here. This is the sum of all the words in the vocabulary but not that we have a count of the word in the query. So in fact we are just taking a sample of query words. This is now a common way that we would use because of its convenience in some transformations. So this is as I said this is sum of all the query words. In our smoothing method we assume that the words that are not observed in the method would have a somewhat different form of probability. Name its four this foru. So were going to do then decompose the sum into two parts. One sum is over all the query words that are matching the document. That means that in this sum all the words have a non zero probability in the document. Sorry its the non zero count of the word in the document. They all occur in the document. And they also have to of course have a non zero count in the query. So these are the query words that are matching the document. On the other hand in this sum we are taking a sum of all the words that are not all query was not matching the document. So they occur in the query due to this term but they dont occur in the document. In this case these words have this probability because of our assumption about the smoothing. That here these seen words have a different probability. Now we can go further by rewriting the second sum as a difference of two other sums. Basically the first sum is the sum of all the query words. Now we know that the original sum is not over all the query words. This is over all the query words that are not matched in the document. So here we pretend that they are actually over all the query words. So we take a sum over all the query words. Obviously this sum has extra terms that are not in this sum. Because here were taking sum over all the query words. There its not matched in the document. So in order to make them equal we will have to then subtract another sum here. And this is the sum over all the query words that are matching in the document. And this makes sense because here we are considering all query words. And then we subtract the query that was matched in the document. That would give us the query that was not matched in the document. And this is almost a reverse process of the first step here. And you might wonder why do we want to do that. Well thats because if we do this then we have different forms of terms inside of these sums. So now you can see in this sum we have all the words matched the query was matching the document with this kind of term. Here we have another sum over the same set of terms matched query terms in document. But inside the sum its different. But these two sums can clearly be merged. So if we do that well get another form of the formula that looks like before me at the bottom here. And note that this is a very interesting formula. Because here we combine these two that all or some of the query words matching in the document in the one sum here. And the other sum now is decomposing into two parts. And these two parts look much simpler just because these are the probabilities of unseen words. This formula is very interesting because you can see the sum is now over the match the query terms. And just like in the vector space model we take a sum of terms that are in the intersection of query vector and the document vector. So it already looks a little bit like the vector space model. In fact theres even more similarity here as we explain on this slide.  So I showed you how we rewrite the query like holder which is a function into a form that looks like the formula of this slide after if we make the assumption about the smoothing the language model based on the collection language model. Now if you look at this rewriting it will actually give us two benefits. The first benefit is it helps us better understand this ranking function. In particular were going to show that from this formula we can see smoothing with the collection language model would give us something like a TF IDF weighting and length normalization. The second benefit is that it also allows us to compute the query like holder more efficiently. In particular we see that the main part of the formula is a sum over the match of the query terms. So this is much better than if we take a sum over all the words. After we smooth the document the damage model we essentially have non zero problem for all the words. So this new form of the formula is much easier to score or to compute. Its also interesting to note that the last term here is actually independent of the document. Since our goal is to rank the documents for the same query we can ignore this term for ranking. Because its going to be the same for all the documents. Ignoring it wouldnt affect the order of the documents. Inside the sum we also see that each matched query term would contribute a weight. And this weight actually is very interesting because it looks like a TF IDF weighting. First we can already see it has a frequency of the word in a query just like in the vector space model. When we take a thought product we see the word frequency in the query to show up in such a sum. And so naturally this part would correspond between the vector element from the documented vector. And here indeed we can see it actually encodes a weight that has similar in factor to TF IDF weight. Ill let you examine it can you see it? Can you see which part is capturing TF? And which part is a capturing IDF weighting? So if want you can pause the video to think more about it. So have you noticed that this P sub seen is related to the term frequency in the sense that if a word occurs very frequently in the document then the s made through probability here will tend to be larger. So this means this term is really doing something like a TF weight. Now have you also noticed that this term in the denominator is actually achieving the factor of IDF? Why because this is the popularity of the term in a collection. But its in the denominator so if the probability in the collection is larger then the weight is actually smaller. And this means a popular term. We actually have a smaller weight and this is precisely what IDF weighting is doing. Only that we now have a different form of TF and IDF. Remember IDF has a logarithm of documented frequency. But here we have something different. But intuitively it achieves a similar effect. Interestingly we also have something related to the length of libation. Again can you see which factor is related to the document length in this formula? What I just say is that this term is related to IDF weighting. This collection probability but it turns out that this term here is actually related to document length normalization. In particular F of sub d might be related to document length. So it encodes how much probability mass we want to give to unseen worlds. How much smoothing do we want to do? Intuitively if a document is long then we need to do less smoothing because we can assume that data is large enough. We probably have observed all the words that the author could have written. But if the document is short then r of sub t could be expected to be large. We need to do more smoothing. Its likey there are words that have not been written yet by the author. So this term appears to paralyze the non document in that other sub D would tend to be longer than or larger than for a long document. But note that alpha sub d also occurs here and so this may not actually be necessary paralyzing long documents. The effect is not so clear yet. But as we will see later when we consider some specific smoothing methods it turns out that they do paralyze long documents. Just like in TF IDF weighting and document length normalization formula in the vector space model. So thats a very interesting observation because it means we dont even have to think about the specific way of doing smoothing. We just need to assume that if we smooth with this collection memory model then we would have a formula that looks like TF IDF weighting and documents length violation. Whats also interesting that we have very fixed form of the ranking function. And see we have not heuristically put a logarithm here. In fact you can think about why we would have a logarithm here. You look at the assumptions that we have made it would be clear its because we have used a logarithm of query like for scoring. And we turned the product into a sum of logarithm of probability and thats why we have this logarithm. Note that if only want to heuristically implement a TF weighting and IDF weighting we dont necessary have to have a logarithm here. Imagine if we drop this logarithm we would still have TF and IDF weighting. But whats nice with problem risk modeling is that we are automatically given the logarithm function here. And thats basically a fixed form of the formula that we did not really have to heuristically design and in this case if you try to drop the logarithm the model probably wont work as well as if you keep the logarithm. So a nice property of problem risk modeling is that by following some assumptions and the probability rules well get a formula automatically. And the formula would have a particular form like in this case. And if we heuristically design the formula we may not necessarily end up having such a specific formula. So to summarize we talked about the need for smoothing the document imaging model. Otherwise it would give zero probability for unseen words in the document and thats not good for storing a query with such an unseen word. Its also necessary in general to improve the accuracy of estimating the model represent the topic of this document. The general idea of smoothing in retrieval is to use the connecting memory model to to give us some clue about which unseen words should have a higher probability. That is the probability of an unseen word is assumed to be proportional to its probability in the collection. With this assumption weve shown that we can derive a general ranking formula for query likelihood that has effect of TF IDF weighting and document length normalization. We also see that through some rewriting the scoring of such a ranking function is primarily based on sum of weights on matched query terms just like in the vector space model. But the actual ranking function is given us automatically by the probability rules and assumptions that we have made. And like in the vector space model where we have to heuristically think about the form of the function. However we still need to address the question how exactly we should smooth the document and the model. How exactly we should use the reference and model based on the connection to adjust the probability of the maximum micro is made of and this is the topic of the next batch.  This lecture is about the specific smoothing methods for language models used in probabilistic retrieval model. In this lecture we will continue the discussion of language models for information retrieval particularly the query likelihood retrieval method. And were going to talk about specifically the smoothing methods used for such a retrieval function. So this is a slide from a previous lecture where we show that with a query likelihood ranking and smoothing with the collection language model we add up having a retrieval function that looks like the following. So this is the retrieval function based on these assumptions that we have discussed. You can see its a sum of all the matching query terms here. And inside its sum is the count of the term in the query and some weight for the term in the document. We have t of i the f weight here and then we have another constant here in n. So clearly if we want to implement this function using programming language we still need to figure out a few variables. In particular were going to need to know how to estimate the probability of a word exactly and how do we set alpha. So in order to answer this question we have to think about very specific smoothing methods and that is main topic of this lecture. Were going to talk about two smoothing methods. The first is simple linear interpolation with a fixed coefficient. And this is also called a Jelinek Mercer smoothing. So the idea is actually very simple. This picture shows how we estimate a document language model by using maximum likelihood estimate. That gives us word counts normalized by the total number of words in the text. The idea of using this method is to maximize the probability of the observed text. As a result if a word like network is not observed in the text its going to get 0 probability as shown here. So the idea of smoothing then is to rely on collection language model where this word is not going to have a zero probability to help us decide what nonzero probability should be assigned to such a word. So we can note that network has a nonzero probability here. So in this approach what we do is we do a linear interpolation between the maximum likelihood placement here and the collection language model and this is computed by the smoothing parameter lambda which is between 0 and 1. So this is a smoothing parameter. The larger lambda is the more smoothing we will have. So by mixing them together we achieve the goal of assigning nonzero probabilities to a word like network. So lets see how it works for some of the words here. For example if we compute the smooth probability for text. Now the maximum likelihood estimated gives us 10 over 100 and thats going to be here. But the collection probability is this. So well just combine them together with this simple formula. We can also see the word network which used to have a zero probability now is getting a non zero probability of this value. And thats because the count is going to be zero for network here. But this part is nonzero and thats basically how this method works. Now if you think about this and you can easily see now the alpha sub d in this smoothing method is basically lambda. Because thats remember the coefficient in front of the probability of the word given by the collection language model here. Okay so this is the first smoothing method. The second one is similar but it has a tie in into the coefficient for linear interpolation. Its often called Dirichlet Prior or Bayesian Smoothing. So again here we face problem of zero probability for an unseen word like network. Again we will use the collection language model but in this case were going to combine them in somewhat different ways. The formula first can be seen as a interpolation of the maximum likelihood estimate and the collection language model as before as in the J M smoothing method. Only that the coefficient now is not lambda a fixed number but a dynamic coefficient in this form where mu is a parameter its a non negative value. And you can see if we set mu to a constant the effect is that a long document would actually get a smaller coefficient here. Because a long document will have longer lengths therefore the coefficient is actually smaller. And so a long document would have less smoothing as we would expect. So this seems to make more sense than a fixed coefficient smoothing. Of course this part would be of this form so that the two coefficients would sum to 1. Now this is one way to understand this smoothing. Basically it means its a dynamic coefficient interpolation. There is another way to understand this formula which is even easier to remember and thats on this side. So its easier to see how we can rewrite the smoothing method in this form. Now in this form we can easily see what change we have made to the maximum likelihood estimate which would be this part. So normalize the count by the document length. So in this form we can see what we did is we add this to the count of every word. So what does this mean? Well this is basically something related to the probability of the word in the collection. And we multiply that by the parameter mu. And when we combine this with the count here essentially we are adding pseudocounts to the observed text. We pretend every word has got this many pseudocount. So the total count would be the sum of these pseudocounts and the actual count of the word in the document. As a result in total we would have added this many pseudocounts. Why? Because if you take somewhat this one over all the words then well see the probability of the words would sum to 1 and that gives us just mu. So this is the total number of pseudocounts that we added. And so these probabilities would still sum to 1. So in this case we can easily see the method is essentially to add this as a pseudocount to this data. Pretend we actually augment the data by including some pseudo data defined by the collection language model. As a result we have more counts is that the total counts for a word would be like this. And as a result even if a word has zero count here lets say if we have zero count here then it would still have nonzero count because of this part. So this is how this method works. Lets also take a look at some specific example here. So for text again we will have 10 as the original count that we actually observe but we also add some pseudocount. And so the probability of text would be of this form. Naturally the probability of network would be just this part. And so here you can also see whats alpha sub d here. Can you see it? If you want to think about it you can pause the video. But youll notice that this part is basically alpha sub d. So we can see in this case alpha sub d does depend on the document because this length depends on the document whereas in the linear interpolation the J M smoothing method this is a constant.  So lets plug in these model masses into the ranking function to see what we will get okay? This is a general smoothing. So a general ranking function for smoothing with subtraction and you have seen this before. And now we have a very specific smoothing method the JM smoothing method. So now lets see what whats a value for office of D here. And whats the value for p sub c here? Right so we may need to decide this in order to figure out the exact form of the ranking function. And we also need to figure out of course alpha. So lets see. Well this ratio is basically this right so here this is the probability of c board on the top and this is the probability of unseen war or in other words basically 11 times basically the alpha here this so its easy to see that. This can be then rewritten as this. Very simple. So we can plug this into here. And then here whats the value for alpha? What do you think? So it would be just lambda right? And what would happen if we plug in this value here if this is lambda. What can we say about this? Does it depend on the document? No so it can be ignored. Right? So well end up having this ranking function shown here. And in this case you can easy to see this a precisely a vector space model because this part is a sum over all the matched query terms this is an element of the query map. What do you think is a element of the document up there? Well its this right. So thats our document left element. And lets further examine whats inside of this logarithm. Well one plus this. So its going to be nonnegative this log of this its going to be at least 1 right? And these this is a parameter so lambda is parameter. And lets look at this. Now this is a TF. Now we see very clearly this TF weighting here. And the larger the count is the higher the weighting will be. We also see IDF weighting which is given by this. And we see docking the lans relationship here. So all these heuristics are captured in this formula. Whats interesting that we kind of have got this weighting function automatically by making various assumptions. Whereas in the vector space model we had to go through those heuristic design in order to get this. And in this case note that theres a specific form. And when you see whether this form actually makes sense. All right so what do you think is the denominator here hm? This is a math of document. Total number of words multiplied by the probability of the word given by the collection right? So this actually can be interpreted as expected account over word. If were going to draw a word from the connection that we model. And were going to draw as many as the number of words in the document. If you do that the expected account of a word w would be precisely given by this denominator. So this ratio basically is comparing the actual count here. The actual count of the word in the document with expected count given by this product if the word is in fact following the distribution in the clutch this. And if this counter is larger than the expected counter in this part this ratio would be larger than one. So thats actually a very interesting interpretation right? Its very natural and intuitive it makes a lot of sense. And this is one advantage of using this kind of probabilistic reasoning where we have made explicit assumptions. And we know precisely why we have a logarithm here. And why we have these probabilities here. And we also have a formula that intuitively makes a lot of sense and does TF IDF weighting and documenting and some others. Lets look at the the Dirichlet Prior Smoothing. Its very similar to the case of JM smoothing. In this case the smoothing parameter is mu and thats different from lambda that we saw before. But the format looks very similar. The form of the function looks very similar. So we still have linear operation here. And when we compute this ratio one will find that is that the ratio is equal to this. And whats interesting here is that we are doing another comparison here now. Were comparing the actual count. Which is the expected account of the world if we sampled meal worlds according to the collection world probability. So note that its interesting we dont even see docking the lens here and lighter in the JMs model. All right so this of course should be plugged into this part. So you might wonder so where is docking lens. Interestingly the docking lens is here in alpha sub d so this would be plugged into this part. As a result what we get is the following function here and this is again a sum over all the match query words. And were against the queer the query time frequency here. And you can interpret this as the element of a document vector but this is no longer a single dot product right? Because we have this part I know that n is the name of the query right? So that just means if we score this function we have to take a sum over all the query words and then do some adjustment of the score based on the document. But its still its still clear that it does documents lens modulation because this lens is in the denominator so a longer document will have a lower weight here. And we can also see it has tf here and now idf. Only that this time the form of the formula is different from the previous one in JMs one. But intuitively it still implements TFIDF waiting and document lens rendition again the form of the function is dictated by the probabilistic reasoning and assumptions that we have made. Now there are also disadvantages of this approach. And that is theres no guarantee that theres such a form of the formula will actually work well. So if we look about at this geo function all those TF IDF waiting and document lens rendition for example its unclear whether we have sub linear transformation. Unfortunately we can see here there is a logarithm function here. So we do have also the so its here right? So we do have the sublinear transformation but we do not intentionally do that. That means theres no guarantee that we will end up in this in this way. Suppose we dont have logarithm then theres no sub linear transformation. As we discussed before perhaps the formula is not going to work so well. So thats an example of the gap between a formal model like this and the relevance that we have to model which is really a subject motion that is tied to users. So it doesnt mean we cannot fix this. For example imagine if we did not have this logarithm right? So we can take a risk and were going to add one or we can even add double logarithm. But then it would mean that the function is no longer a proper risk model. So the consequence of the modification is no longer as predictable as what we have been doing now. So thats also why for example PM45 remains very competitive and still open channel how to use public risk models as they arrive better model than the PM25. In particular how do we use query like how to derive a model and that would work consistently better than DM 25. Currently we still cannot do that. Still interesting open question. So to summarize this part weve talked about the two smoothing methods. Jelinek Mercer which is doing the fixed coefficient linear interpolation. Dirichlet Prior this is what add a pseudo counts to every word and is doing adaptive interpolation in that the coefficient would be larger for shorter documents. In most cases we can see by using these smoothing methods we will be able to reach a retrieval function where the assumptions are clearly articulate. So they are less heuristic. Explaining the results also show that these retrieval functions. Also are very effective and they are comparable to BM 25 or pm lens adultation. So this is a major advantage of probably smaller where we dont have to do a lot of heuristic design. Yet in the end that we naturally implemented TF IDF weighting and doc length normalization. Each of these functions also has precise ones smoothing parameter. In this case of course we still need to set this smoothing parameter. There are also methods that can be used to estimate these parameters. So overall this shows by using a probabilistic model we follow very different strategies then the vector space model. Yet in the end we end up uhwith some retrievable functions that look very similar to the vector space model. With some advantages in having assumptions clearly stated. And then the form dictated by a probabilistic model. Now this also concludes our discussion of the query likelihood probabilistic model. And lets recall what assumptions we have made in order to derive the functions that we have seen in this lecture. Well we basically have made four assumptions that I listed here. The first assumption is that the relevance can be modeled by the query likelihood. And the second assumption with med is are query words are generated independently that allows us to decompose the probability of the whole query into a product of probabilities of old words in the query. And then the third assumption that we have made is if a word is not seen the document or in the late its probability proportional to its probability in the collection. Thats a smoothing with a collection ama model. And finally we made one of these two assumptions about the smoothing. So we either used JM smoothing or Dirichlet prior smoothing. If we make these four assumptions then we have no choice but to take the form of the retrieval function that we have seen earlier. Fortunately the function has a nice property in that it implements TF IDF weighting and document machine and these functions also work very well. So in that sense these functions are less heuristic compared with the vector space model. And there are many extensions of this this basic model and you can find the discussion of them in the reference at the end of this lecture.